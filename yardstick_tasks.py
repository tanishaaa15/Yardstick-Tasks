# -*- coding: utf-8 -*-
"""Yardstick_Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aq3HvTTwSU8_mGXdaDLo1U0YEjPknQDz

# **Task 1:Managing Conversation History with Summarization**
"""

# Install necessary client (if not already installed)
!pip install openai

import os
import openai
from typing import List, Dict

# --- Setup & Initialization ---
# Set your Groq API key here securely for demonstration purposes.
# In production, use environment variables or input prompts instead.
os.environ["OPENAI_API_KEY"] = "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"


client = openai.OpenAI(
    api_key=os.environ["OPENAI_API_KEY"],
    base_url="https://api.groq.com/openai/v1"
)

# --- Conversation Storage and Truncation Class ---

class ConversationHistory:
    """
    Maintains a running history of conversation messages between user and assistant.
    Supports truncation by number of turns, characters, or words.
    """

    def __init__(self):
        self.messages: List[Dict[str, str]] = []

    def add_message(self, role: str, content: str):
        """
        Adds a new message to the conversation history.
        :param role: 'user' or 'assistant' or 'system'
        :param content: Message text content
        """
        self.messages.append({"role": role, "content": content})

    def truncate_by_turns(self, n: int) -> List[Dict[str, str]]:
        """Return the last n messages."""
        return self.messages[-n:]

    def truncate_by_char(self, max_chars: int) -> List[Dict[str, str]]:
        """
        Return conversation truncated to max_chars characters by iterating
        backwards and collecting messages without exceeding max_chars.
        """
        char_count = 0
        selected = []
        for msg in reversed(self.messages):
            if char_count + len(msg["content"]) > max_chars:
                break
            selected.insert(0, msg)
            char_count += len(msg["content"])
        return selected

    def truncate_by_word(self, max_words: int) -> List[Dict[str, str]]:
        """
        Return conversation truncated to max_words words by iterating
        backwards and collecting messages without exceeding that limit.
        """
        word_count = 0
        selected = []
        for msg in reversed(self.messages):
            msg_words = len(msg["content"].split())
            if word_count + msg_words > max_words:
                break
            selected.insert(0, msg)
            word_count += msg_words
        return selected

# --- Summarization Function with Groq API ---

def summarize_conversation(client, messages: List[Dict[str, str]], instruction: str = "Summarize the conversation so far."):
    """
    Use Groq API OpenAI-compatible chat completions to summarize the conversation.
    :param client: OpenAI-compatible client instance
    :param messages: List of messages (dicts with role, content)
    :param instruction: System instruction to guide summary
    :return: Summary text string
    """
    prompt = messages + [{"role": "system", "content": instruction}]
    try:
        response = client.chat.completions.create(
            model="openai/gpt-oss-120b",  # Replace with your accessible Groq chat model
            messages=prompt,
            max_tokens=150,
            temperature=0.5
        )
        summary_text = response.choices[0].message.content
        return summary_text
    except Exception as e:
        print("Error during summarization API call:", e)
        return "Summary generation failed."

# --- Periodic Summarization Trigger Function ---

def periodic_summary(history: ConversationHistory, period: int, method: str = "turns", n: int = 6):
    """
    Perform summarization after every 'period' messages are added,
    using specified truncate method and parameter n.
    Summary replaces earlier messages with a system summary message.
    """
    if len(history.messages) % period == 0 and len(history.messages) > 0:
        if method == "turns":
            truncated = history.truncate_by_turns(n)
        elif method == "chars":
            truncated = history.truncate_by_char(n)
        elif method == "words":
            truncated = history.truncate_by_word(n)
        else:
            truncated = history.messages

        summary = summarize_conversation(client, truncated)
        # Update conversation with system summary + last half of truncated messages
        history.messages = [{"role": "system", "content": "Summary: " + summary}] + truncated[-(n // 2):]

# --- Demonstration with Sample Conversations ---

if __name__ == "__main__":
    # Initialize conversation history instance
    history = ConversationHistory()

    # Sample user-assistant chat pairs to feed in sequence
    user_assistant_pairs = [
        ("Hello!", "Hi! How can I assist you?"),
        ("What's the weather today?", "It's sunny and 30Â°C in your city."),
        ("Any movie recommendations?", "Try 'Inception' or 'Interstellar'."),
        ("Tell me a joke.", "Why did the computer show up late? It had a hard drive!"),
        ("How do I bake a cake?", "Here's a simple recipe: ..."),
    ]

    # Set summarization period and truncation parameter
    k = 3  # summarize after every 3rd message addition
    n = 6  # keep last 6 messages for truncation and summary context

    # Feed the conversation and apply periodic summarization
    for i, (user_msg, assistant_msg) in enumerate(user_assistant_pairs):
        history.add_message("user", user_msg)
        history.add_message("assistant", assistant_msg)
        periodic_summary(history, period=k, method="turns", n=n)

        # Print the conversation history state after each user-assistant pair
        print(f"\n[After step {i + 1}] Conversation History:")
        for msg in history.messages:
            print(f"{msg['role'].capitalize()}: {msg['content']}")

"""## **Task 2: JSON Schema Classification & Information Extraction**"""

import json
from jsonschema import validate, ValidationError

# --- JSON Schema for user information extraction ---
json_schema = {
    "type": "object",
    "properties": {
        "name": {"type": "string", "description": "User's full name"},
        "email": {"type": "string", "description": "User's email address"},
        "phone": {"type": "string", "description": "User's phone number"},
        "location": {"type": "string", "description": "User's city or location"},
        "age": {"type": "string", "description": "User's age"}
    },
    "required": ["name", "email", "phone", "location", "age"]
}

# --- Function calling definition according to Groq API spec ---
function_def = [{
    "type": "function",  # Groq API requires this property at top level
    "function": {
        "name": "extract_user_info",
        "description": "Extract user's name, email, phone, location, and age from chat.",
        "parameters": json_schema
    }
}]

# --- Sample user chat inputs ---
sample_chats = [
    "Hi, I am Rahul Sharma from Delhi. My email is rahul.sharma@gmail.com and phone number is 9876543210. I am 21 years old.",
    "My name is Priya Verma, you can reach me at verma.priya@email.com. I'm based in Bangalore, 25 years old, and my phone is 9988776655.",
    "Hello, contact me at aman_1999@gmail.com, age 24, number 7788990011, Jaipur."
]

def extract_and_validate(chat_text):
    """
    Uses Groq API with function calling to extract structured user info.
    Then validates the extracted data against the JSON schema.
    Prints extracted info and validation results.
    """
    try:
        response = client.chat.completions.create(
            model="llama-3.3-70b-versatile",  # Replace with your Groq model from access list
            messages=[{"role": "user", "content": chat_text}],
            tools=function_def,
            tool_choice={"type": "function", "function": {"name": "extract_user_info"}},
            max_tokens=200,
            temperature=0,
        )
        # Get the function call output arguments as JSON string and parse it
        tool_call = response.choices[0].message.tool_calls[0]
        extracted_data = json.loads(tool_call.function.arguments)

        # Validate extracted JSON against schema
        validate(instance=extracted_data, schema=json_schema)

        print(f"Chat:\n{chat_text}")
        print("Extracted Info:", extracted_data)
        print("Validation: PASS\n")

    except ValidationError as ve:
        print(f"Chat:\n{chat_text}")
        print("Validation Error:", ve.message, "\n")

    except Exception as e:
        print(f"Chat:\n{chat_text}")
        print("Extraction failed:", str(e), "\n")

# Run extraction and validation on sample chats
for chat in sample_chats:
    extract_and_validate(chat)